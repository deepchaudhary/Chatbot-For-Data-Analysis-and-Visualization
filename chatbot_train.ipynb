{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHxRqZuk0rHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nus2-8qfCdwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhVYOexxH4Mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhVr6Nj0ICVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c11XhqrgIf4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdS6Noq7iAk-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3a70467c-a7a7-43da-b325-7dad99571cf2"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YztgByAsmsla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCcPDXyZnciB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "db906293-a609-45a7-bab8-34e9c3dbede6"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SW_PDj_nMdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ignore_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ7mKE6KoFWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73abb997-0f20-4906-e01a-9908ead60584"
      },
      "source": [
        "ignore_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU5bgP22mAqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMp1Nr2x21lR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ooYYspiGcf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file=open(r'dashboard.json').read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05rohjyeHUJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intents=json.loads(data_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1VuAKaCefZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c2510c4-6fe2-4437-f410-abb54397af12"
      },
      "source": [
        "intents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'context': [''],\n",
              "   'patterns': ['open dashboard',\n",
              "    'show dashboard',\n",
              "    'display dashboard',\n",
              "    'enter dashboard',\n",
              "    'prepare dashboard'],\n",
              "   'responses': ['Dynamic Dashboard Opened in a new Window'],\n",
              "   'tag': 'Open_Dashboard'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['what is the shape of dataset',\n",
              "    'what is size of dataset',\n",
              "    'lenght of dataset'],\n",
              "   'responses': ['Here is the size of dataset:'],\n",
              "   'tag': 'shape'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['what are the columns of dataset',\n",
              "    'show the dataset attributes',\n",
              "    'fields in dataset',\n",
              "    'give the columns of dataset'],\n",
              "   'responses': ['Here are the columns of the dataset:'],\n",
              "   'tag': 'columns'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['plot line chart',\n",
              "    'show line graph',\n",
              "    'draw line graph',\n",
              "    'give line plot'],\n",
              "   'responses': ['Line plot opened in a new window:'],\n",
              "   'tag': 'lineplot'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['what is this dataset about',\n",
              "    'tell me something about the dataset',\n",
              "    'what data do you have',\n",
              "    'give some information about dataset'],\n",
              "   'responses': ['We are analyzing tata motors car sales',\n",
              "    'I have dataset of Tata Motors car sales',\n",
              "    'The dataset is about the car sales of Tata Motors'],\n",
              "   'tag': 'dataset'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['give summary of data',\n",
              "    'give details about the data',\n",
              "    'describe the data',\n",
              "    'show data description'],\n",
              "   'responses': ['A detailed summary of data is given below',\n",
              "    'Data is described below',\n",
              "    'Here is the summary about the data'],\n",
              "   'tag': 'summary'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['plot a bar graph',\n",
              "    'draw a bar graph',\n",
              "    'show bar plot',\n",
              "    'display bar chart'],\n",
              "   'responses': ['Bar graph opened in a new window'],\n",
              "   'tag': 'barplot'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['select dataset',\n",
              "    'input dataset',\n",
              "    'take my dataset',\n",
              "    'choose the data',\n",
              "    'load dataset'],\n",
              "   'responses': ['Input your dataset directory'],\n",
              "   'tag': 'selectdataset'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['F:\\\\data sets\\\\ttl dashboards\\\\Consolidated Live Retail Data.xlsx',\n",
              "    'F:\\\\data sets\\\\iris-species\\\\Iris.xlsx'],\n",
              "   'responses': [\"Your dataset loaded. Tell me What's next?\"],\n",
              "   'tag': 'datadir'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['show top dealers',\n",
              "    'top dealers',\n",
              "    'display top dealers',\n",
              "    'find top dealers',\n",
              "    'give top dealers'],\n",
              "   'responses': ['top dealers'],\n",
              "   'tag': 'Top_dealers'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['Hi there',\n",
              "    'Is anyone there?',\n",
              "    'Hey',\n",
              "    'Hola',\n",
              "    'Hello',\n",
              "    'Good day'],\n",
              "   'responses': ['Hello, pleasure to meet you.',\n",
              "    'Good to see you again',\n",
              "    'Hi there, how can I help?'],\n",
              "   'tag': 'greeting'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['Bye',\n",
              "    'See you later',\n",
              "    'Goodbye',\n",
              "    'Nice chatting to you, bye',\n",
              "    'Till next time'],\n",
              "   'responses': ['See you!', 'Have a nice day', 'Bye! Come back again soon.'],\n",
              "   'tag': 'goodbye'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['Good',\n",
              "    'Great',\n",
              "    'Fine',\n",
              "    'Awesome',\n",
              "    'Nice work',\n",
              "    'Any time',\n",
              "    'Welcome'],\n",
              "   'responses': ['Good to hear that.'],\n",
              "   'tag': 'great'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['How are you ?', 'How are you doing', 'Is all well there'],\n",
              "   'responses': ['I am good. How about you ?',\n",
              "    'All well here. Thanks for asking'],\n",
              "   'tag': 'wellbeing'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['Thanks',\n",
              "    'Thank you',\n",
              "    \"That's helpful\",\n",
              "    'Awesome, thanks',\n",
              "    'Thanks for helping me'],\n",
              "   'responses': ['Happy to help!', 'Any time!', 'My pleasure'],\n",
              "   'tag': 'thanks'},\n",
              "  {'context': [''],\n",
              "   'patterns': [],\n",
              "   'responses': [\"Sorry, can't understand you\",\n",
              "    'Please give me more info',\n",
              "    'Not sure I understand'],\n",
              "   'tag': 'noanswer'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['how old',\n",
              "    'how old is ad-hocBot',\n",
              "    'what is your age',\n",
              "    'how old are you',\n",
              "    'age?'],\n",
              "   'responses': ['I am month old!',\n",
              "    'My birth year 2019',\n",
              "    'I am not too much old i am only one month old'],\n",
              "   'tag': 'age'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['what is your name',\n",
              "    'what should I call you',\n",
              "    'whats your name?',\n",
              "    ''],\n",
              "   'responses': ['You can call me VisBot.',\n",
              "    \"I'm VisBot!\",\n",
              "    \"I'm VisBot from SRM IST.\"],\n",
              "   'tag': 'name'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['address',\n",
              "    'location',\n",
              "    'what is address',\n",
              "    'where do you stay',\n",
              "    'what is your location'],\n",
              "   'responses': ['E-40, Kings St, Chennai, India'],\n",
              "   'tag': 'address'},\n",
              "  {'context': [''],\n",
              "   'patterns': ['How you could help me?',\n",
              "    'What you can do?',\n",
              "    'What help you provide?',\n",
              "    'How you can be helpful?',\n",
              "    'What support is offered'],\n",
              "   'responses': ['I can help in Data Analysis and Data Visualization'],\n",
              "   'tag': 'options'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqv5FSCjgqFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words=[]\n",
        "documents=[]\n",
        "classes=[]\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    w=nltk.word_tokenize(pattern)\n",
        "    words.extend(w)\n",
        "    documents.append((w,intent['tag']))\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCcFqP3uhlWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCK86iLIobNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = sorted(list(set(classes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpRugD-ltyct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "84c54edf-2c05-490f-906b-292bbcafb120"
      },
      "source": [
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique lemmatized words\", words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85 documents\n",
            "19 classes ['Open_Dashboard', 'Top_dealers', 'address', 'age', 'barplot', 'columns', 'datadir', 'dataset', 'goodbye', 'great', 'greeting', 'lineplot', 'name', 'options', 'selectdataset', 'shape', 'summary', 'thanks', 'wellbeing']\n",
            "94 unique lemmatized words [\"'s\", ',', ':', '?', '\\\\data', 'ad-hocbot', 'address', 'age', 'any', 'anyone', 'attribute', 'awesome', 'bar', 'bye', 'call', 'chart', 'chatting', 'choose', 'column', 'could', 'dashboard', 'dashboards\\\\consolidated', 'data', 'data.xlsx', 'dataset', 'day', 'dealer', 'describe', 'description', 'detail', 'display', 'draw', 'enter', 'f', 'field', 'find', 'fine', 'give', 'good', 'goodbye', 'graph', 'great', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'hola', 'how', 'i', 'information', 'input', 'is', 'later', 'lenght', 'line', 'live', 'load', 'location', 'name', 'next', 'nice', 'offered', 'old', 'open', 'plot', 'prepare', 'provide', 'retail', 'see', 'select', 'sets\\\\iris-species\\\\iris.xlsx', 'sets\\\\ttl', 'shape', 'show', 'size', 'something', 'stay', 'summary', 'support', 'take', 'tell', 'thank', 'thanks', 'that', 'till', 'time', 'top', 'welcome', 'well', 'what', 'whats', 'work']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB_bN0g8B9TO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(words,open(r'words.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E16JnE8yCYQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(classes,open(r'classes.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h51wygZSCqX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "for doc in documents:\n",
        "  bag = []\n",
        "  pattern_words = doc[0]\n",
        "  pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "  for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "  training.append([bag, output_row])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMNH3bxTF3Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.shuffle(training)\n",
        "training = np.array(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EofZ1k59Hz32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUqzgus-IWAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlQuuhdTLi78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTEjn-veM5nA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14e79dd7-2695-476b-cb23-90a058796d34"
      },
      "source": [
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 2.9621 - accuracy: 0.0235\n",
            "Epoch 2/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 2.9356 - accuracy: 0.0353\n",
            "Epoch 3/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 2.8368 - accuracy: 0.1294\n",
            "Epoch 4/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 2.8504 - accuracy: 0.1176\n",
            "Epoch 5/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 2.7362 - accuracy: 0.2471\n",
            "Epoch 6/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 2.6780 - accuracy: 0.2353\n",
            "Epoch 7/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 2.5745 - accuracy: 0.2941\n",
            "Epoch 8/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 2.4372 - accuracy: 0.2824\n",
            "Epoch 9/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 2.3897 - accuracy: 0.3176\n",
            "Epoch 10/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 2.3485 - accuracy: 0.3529\n",
            "Epoch 11/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 2.1833 - accuracy: 0.4000\n",
            "Epoch 12/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 1.9498 - accuracy: 0.4824\n",
            "Epoch 13/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 2.0438 - accuracy: 0.4706\n",
            "Epoch 14/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.8439 - accuracy: 0.4706\n",
            "Epoch 15/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.6641 - accuracy: 0.5412\n",
            "Epoch 16/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.5729 - accuracy: 0.4824\n",
            "Epoch 17/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.3612 - accuracy: 0.6235\n",
            "Epoch 18/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.2988 - accuracy: 0.6235\n",
            "Epoch 19/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.3783 - accuracy: 0.5882\n",
            "Epoch 20/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.2963 - accuracy: 0.5882\n",
            "Epoch 21/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.2476 - accuracy: 0.6824\n",
            "Epoch 22/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.0939 - accuracy: 0.6353\n",
            "Epoch 23/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.1881 - accuracy: 0.6235\n",
            "Epoch 24/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 1.1338 - accuracy: 0.6706\n",
            "Epoch 25/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.0023 - accuracy: 0.7059\n",
            "Epoch 26/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.9820 - accuracy: 0.7529\n",
            "Epoch 27/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 1.0361 - accuracy: 0.6353\n",
            "Epoch 28/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.8629 - accuracy: 0.7294\n",
            "Epoch 29/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.7890 - accuracy: 0.7647\n",
            "Epoch 30/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.7882 - accuracy: 0.7765\n",
            "Epoch 31/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.8349 - accuracy: 0.7647\n",
            "Epoch 32/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6842 - accuracy: 0.7765\n",
            "Epoch 33/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6687 - accuracy: 0.8353\n",
            "Epoch 34/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.7304 - accuracy: 0.7412\n",
            "Epoch 35/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.7897 - accuracy: 0.7647\n",
            "Epoch 36/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6287 - accuracy: 0.8000\n",
            "Epoch 37/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.5567 - accuracy: 0.8471\n",
            "Epoch 38/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.6433 - accuracy: 0.8000\n",
            "Epoch 39/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5943 - accuracy: 0.8471\n",
            "Epoch 40/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.8353\n",
            "Epoch 41/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.5339 - accuracy: 0.8353\n",
            "Epoch 42/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.9059\n",
            "Epoch 43/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4555 - accuracy: 0.8941\n",
            "Epoch 44/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3630 - accuracy: 0.9059\n",
            "Epoch 45/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.3608 - accuracy: 0.8941\n",
            "Epoch 46/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4667 - accuracy: 0.8353\n",
            "Epoch 47/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.8471\n",
            "Epoch 48/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.8941\n",
            "Epoch 49/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.8824\n",
            "Epoch 50/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.8471\n",
            "Epoch 51/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.9176\n",
            "Epoch 52/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.4172 - accuracy: 0.8941\n",
            "Epoch 53/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8824\n",
            "Epoch 54/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.9647\n",
            "Epoch 55/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2475 - accuracy: 0.9059\n",
            "Epoch 56/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2430 - accuracy: 0.9412\n",
            "Epoch 57/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8706\n",
            "Epoch 58/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.8706\n",
            "Epoch 59/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3288 - accuracy: 0.9059\n",
            "Epoch 60/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.9176\n",
            "Epoch 61/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.3837 - accuracy: 0.8824\n",
            "Epoch 62/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.3488 - accuracy: 0.8588\n",
            "Epoch 63/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2957 - accuracy: 0.9176\n",
            "Epoch 64/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9529\n",
            "Epoch 65/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2713 - accuracy: 0.9059\n",
            "Epoch 66/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8706\n",
            "Epoch 67/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.9529\n",
            "Epoch 68/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2285 - accuracy: 0.9529\n",
            "Epoch 69/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.9412\n",
            "Epoch 70/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9647\n",
            "Epoch 71/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1728 - accuracy: 0.9765\n",
            "Epoch 72/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2391 - accuracy: 0.9529\n",
            "Epoch 73/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2305 - accuracy: 0.8941\n",
            "Epoch 74/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2776 - accuracy: 0.9176\n",
            "Epoch 75/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9412\n",
            "Epoch 76/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2355 - accuracy: 0.9529\n",
            "Epoch 77/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.9059\n",
            "Epoch 78/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.9059\n",
            "Epoch 79/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9412\n",
            "Epoch 80/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1984 - accuracy: 0.9529\n",
            "Epoch 81/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2220 - accuracy: 0.9294\n",
            "Epoch 82/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.4057 - accuracy: 0.8941\n",
            "Epoch 83/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2576 - accuracy: 0.9294\n",
            "Epoch 84/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2572 - accuracy: 0.8941\n",
            "Epoch 85/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1554 - accuracy: 0.9529\n",
            "Epoch 86/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.9412\n",
            "Epoch 87/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1148 - accuracy: 0.9765\n",
            "Epoch 88/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.9412\n",
            "Epoch 89/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9647\n",
            "Epoch 90/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9647\n",
            "Epoch 91/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1688 - accuracy: 0.9529\n",
            "Epoch 92/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9647\n",
            "Epoch 93/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1380 - accuracy: 0.9765\n",
            "Epoch 94/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9294\n",
            "Epoch 95/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9647\n",
            "Epoch 96/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9765\n",
            "Epoch 97/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1692 - accuracy: 0.9647\n",
            "Epoch 98/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9647\n",
            "Epoch 99/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1853 - accuracy: 0.9412\n",
            "Epoch 100/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.2333 - accuracy: 0.9412\n",
            "Epoch 101/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9529\n",
            "Epoch 102/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9882\n",
            "Epoch 103/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9647\n",
            "Epoch 104/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9529\n",
            "Epoch 105/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1864 - accuracy: 0.9529\n",
            "Epoch 106/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9765\n",
            "Epoch 107/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9647\n",
            "Epoch 108/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9882\n",
            "Epoch 109/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1681 - accuracy: 0.9412\n",
            "Epoch 110/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9412\n",
            "Epoch 111/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9647\n",
            "Epoch 112/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9529\n",
            "Epoch 113/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9412\n",
            "Epoch 114/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1625 - accuracy: 0.9647\n",
            "Epoch 115/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9529\n",
            "Epoch 116/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9529\n",
            "Epoch 117/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9765\n",
            "Epoch 118/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1574 - accuracy: 0.9529\n",
            "Epoch 119/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9765\n",
            "Epoch 120/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1099 - accuracy: 0.9765\n",
            "Epoch 121/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0999 - accuracy: 0.9765\n",
            "Epoch 122/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9529\n",
            "Epoch 124/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1599 - accuracy: 0.9294\n",
            "Epoch 125/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9647\n",
            "Epoch 126/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1137 - accuracy: 0.9647\n",
            "Epoch 127/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9412\n",
            "Epoch 128/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9647\n",
            "Epoch 129/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9294\n",
            "Epoch 130/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9529\n",
            "Epoch 131/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9412\n",
            "Epoch 132/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9529\n",
            "Epoch 133/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9882\n",
            "Epoch 134/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0952 - accuracy: 0.9882\n",
            "Epoch 135/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9647\n",
            "Epoch 136/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9882\n",
            "Epoch 137/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9765\n",
            "Epoch 139/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9529\n",
            "Epoch 140/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1048 - accuracy: 0.9765\n",
            "Epoch 141/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0709 - accuracy: 0.9882\n",
            "Epoch 142/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0907 - accuracy: 0.9529\n",
            "Epoch 143/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9765\n",
            "Epoch 144/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9882\n",
            "Epoch 145/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1521 - accuracy: 0.9529\n",
            "Epoch 147/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0825 - accuracy: 0.9882\n",
            "Epoch 148/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9529\n",
            "Epoch 149/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0418 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0697 - accuracy: 0.9882\n",
            "Epoch 153/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0780 - accuracy: 0.9765\n",
            "Epoch 154/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9765\n",
            "Epoch 155/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 0.9882\n",
            "Epoch 156/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1508 - accuracy: 0.9294\n",
            "Epoch 157/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9765\n",
            "Epoch 158/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9765\n",
            "Epoch 159/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9647\n",
            "Epoch 160/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1816 - accuracy: 0.9529\n",
            "Epoch 161/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.9412\n",
            "Epoch 162/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1069 - accuracy: 0.9647\n",
            "Epoch 163/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.9765\n",
            "Epoch 164/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0960 - accuracy: 0.9765\n",
            "Epoch 165/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0541 - accuracy: 0.9882\n",
            "Epoch 166/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0814 - accuracy: 0.9765\n",
            "Epoch 167/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9647\n",
            "Epoch 168/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9647\n",
            "Epoch 169/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0960 - accuracy: 0.9765\n",
            "Epoch 170/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1728 - accuracy: 0.9529\n",
            "Epoch 171/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9882\n",
            "Epoch 172/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0822 - accuracy: 0.9882\n",
            "Epoch 173/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9765\n",
            "Epoch 176/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.9765\n",
            "Epoch 177/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0972 - accuracy: 0.9882\n",
            "Epoch 178/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0347 - accuracy: 0.9882\n",
            "Epoch 179/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9882\n",
            "Epoch 180/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0464 - accuracy: 0.9882\n",
            "Epoch 181/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9647\n",
            "Epoch 182/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.9882\n",
            "Epoch 184/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0525 - accuracy: 0.9882\n",
            "Epoch 185/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0591 - accuracy: 0.9882\n",
            "Epoch 186/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0246 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9412\n",
            "Epoch 188/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0959 - accuracy: 0.9647\n",
            "Epoch 190/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.1065 - accuracy: 0.9647\n",
            "Epoch 191/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9529\n",
            "Epoch 192/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0925 - accuracy: 0.9647\n",
            "Epoch 193/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9882\n",
            "Epoch 194/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9529\n",
            "Epoch 195/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9882\n",
            "Epoch 196/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9647\n",
            "Epoch 197/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9647\n",
            "Epoch 199/200\n",
            "17/17 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9882\n",
            "Epoch 200/200\n",
            "17/17 [==============================] - 0s 1ms/step - loss: 0.0468 - accuracy: 0.9882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTMUy3REPvWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(r'chatbot_model.h5', hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66pV1Pr8P1um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}